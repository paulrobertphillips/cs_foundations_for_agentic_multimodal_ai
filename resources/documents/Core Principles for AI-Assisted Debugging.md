Great question â€” debugging is one of the **highest-leverage** places to use AI *well*, and also one of the easiest places for things to go wrong if prompts arenâ€™t structured.

Iâ€™ll give you:

1. **Core best practices for AI-assisted debugging**
2. **A data-science debugging prompt (example)**
3. **An architecture / system-level debugging prompt (example)**
4. A short **anti-patterns checklist** (what to avoid)

No overlap with Module 3 governance â€” this is still firmly **Module 2: interaction mechanics**.

---

## 1ï¸âƒ£ Core Principles for AI-Assisted Debugging (Memorize These)

### ðŸ”‘ Principle 1: Debugging â‰  Refactoring

When debugging, your goal is:

* **identify cause**
* **verify hypothesis**
* **apply minimal fix**

Not:

* â€œclean upâ€
* â€œimprove designâ€
* â€œmodernize codeâ€

You must **explicitly forbid refactors** unless you *want* them.

---

### ðŸ”‘ Principle 2: Force Hypothesis Before Fix

AI is very good at proposing fixes.
AI is **less reliable** at identifying root causes unless forced.

So your prompt should always require:

1. probable cause(s)
2. evidence from code/logs
3. confirmation before changes

---

### ðŸ”‘ Principle 3: Minimize the Blast Radius

You should always constrain:

* files allowed to change
* functions allowed to change
* interfaces that must remain identical

This keeps debugging from turning into accidental redesign.

---

### ðŸ”‘ Principle 4: Debug in Two Phases

Just like planning/execution:

* **Phase A: Diagnosis (no code)**
* **Phase B: Fix (minimal diff)**

Never combine them unless the bug is trivial.

---

## 2ï¸âƒ£ Example 1 â€” Data Science Debugging Prompt

### Scenario

A pandas-based pipeline is producing incorrect row counts after a cleaning step.

### âœ… Debugging Prompt (Diagnosis Mode)

```text
You are acting as a junior data scientist debugging a data pipeline.

Context:
- This code previously worked
- A recent change caused incorrect row counts after cleaning
- The pipeline uses pandas and pure functions

Task:
DEBUGGING ONLY â€” do not refactor or redesign.

Instructions:
1) Read the provided code and error/output description
2) Identify the most likely root cause(s)
3) Point to the exact line(s) of code responsible
4) Explain why the behavior occurs
5) Propose the minimal change required to fix it

Constraints:
- Do not change function signatures
- Do not introduce new dependencies
- Do not add new features
- Do not optimize performance

Do NOT write code yet.
Stop after diagnosis and wait for approval.
```

### Why this works

* Forces causal reasoning
* Anchors the AI to *existing behavior*
* Prevents â€œcleanup while debuggingâ€
* Produces a reviewable explanation

---

### âœ… Follow-up Execution Prompt (After Approval)

```text
Using the approved diagnosis:

Apply the minimal fix described.

Constraints:
- Modify only the identified lines
- Preserve all existing behavior outside the bug
- Do not refactor or rename anything

Deliverable:
Code diff only.
```

---

## 3ï¸âƒ£ Example 2 â€” Architecture / System-Level Debugging Prompt

### Scenario

A pipeline runs fine locally but fails when integrated into an application or job runner.

### âœ… Architecture Debugging Prompt (Diagnosis Mode)

```text
You are acting as a junior software engineer debugging a data pipeline integration issue.

Context:
- The pipeline logic is correct and tested
- Failures occur only when invoked via the application entry point
- No recent functional changes were made to the pipeline itself

Task:
DEBUGGING ONLY â€” diagnosis, not redesign.

Instructions:
1) Identify where assumptions between components may be mismatched
2) Trace the flow of inputs, configuration, and outputs across boundaries
3) Identify the most likely failure point(s)
4) Explain why the failure appears only in this execution context

Constraints:
- Do not propose architectural changes
- Do not introduce new frameworks
- Do not modify pipeline logic
- Focus only on configuration, invocation, or interface mismatches

Do NOT write code.
Stop after diagnosis and wait for approval.
```

### Why this works

* Focuses on **interfaces**, not internals
* Prevents AI from â€œfixingâ€ the wrong layer
* Encourages boundary analysis (a common DS pain point)

---

### âœ… Follow-up Execution Prompt

```text
Using the approved diagnosis:

Apply the minimal fix needed to resolve the integration issue.

Constraints:
- Modify only the invocation/configuration layer
- Do not change pipeline behavior
- Preserve existing interfaces

Deliverable:
Code diff only.
```

---

## 4ï¸âƒ£ Common Debugging Anti-Patterns (Avoid These)

### ðŸš« â€œFix this bug and clean up the codeâ€

Invites scope creep + refactor drift.

### ðŸš« â€œImprove error handling while fixingâ€

Error handling changes behavior â€” thatâ€™s not debugging anymore.

### ðŸš« â€œMake it more robustâ€

Ambiguous and architectural.

### ðŸš« Not providing expected vs actual behavior

AI needs *contrast* to reason correctly.

---

## 5ï¸âƒ£ Signals That AI Is Going Off Track While Debugging

Watch for:

* suggestions to rename or reorganize files
* adding abstractions
* performance â€œimprovementsâ€
* changes not directly tied to the bug
* lack of causal explanation (â€œthis should fix itâ€)

When you see that, stop and reset to **diagnosis-only mode**.

---

## The One Debugging Rule to Keep

> **If AI canâ€™t explain *why* a bug occurs, it shouldnâ€™t be allowed to fix it.**

This single rule will prevent ~80% of AI-assisted debugging failures.

---